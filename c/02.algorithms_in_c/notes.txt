# Algorithms in C

## contents
Chapter 1. Introduction 3
1.1 Algorithms . 4
1.2 A Sample Problem-Connectivity· 6
1.3 Union-Find Algorithms . 11
1.4 Perspective . 22
1.5 Summary of Topics· 23
Chapter 2. Principles of Algorithm Analysis 27
2.1 Implementation and Empirical Analysis· 28
2.2 Analysis of Algorithms . 33
2.3 Growth of Functions . 36
2.4 Big-Oh notation· 44
2.5 Basic Recurrences· 49
2.6 Examples of Algorithm Analysis· 53
2.7 Guarantees, Predictions, and Limitations . 59
Chapter 3. Elementary Data Structures 69
3.1 Building Blocks· 70
3.2 Arrays· 82
3.3 Linked Lists· 90
3.4 Elementary List Processing . 96
3.5 Memory Allocation for Lists . 105
3.6 Strings . 109
3.7 Compound Data Structures· 115
Chapter 4. Abstract Data Types 127
4.1 Abstract Objects and Collections of Objects· 131
4.2 Pushdown Stack ADT . 135
4.3 Examples of Stack ADT Clients . 138
4.4 Stack ADT Implementations· 144
4.5 Creation of a New ADT . 149
4.6 FIFO Queues and Generalized Queues . 153
4.7 Duplicate and Index Items· 161
4.8 First-Class ADTs . 165
4.9 Application-Based ADT Example . 178
4.10 Perspective· 184
Chapter 5. Recursion and Trees 187
5.1 Recursive Algorithms· 188
5.2 Divide and Conquer . 196
5.3 Dynamic Programming· 208
5.4 Trees· 216
5.5 Mathematical Properties of Trees· 226
5.6 Tree Traversal· 230
5.7 Recursive Binary-Tree Algorithms· 235
5.8 Graph Traversal· 241
5.9 Perspective· 247
Chapter 6. Elementary Sorting Methods 253
6.1 Rules of the Game· 255
6.2 Selection Sort· 261
6.3 Insertion Sort ' 262
6.4 Bubble Sort· 265
6.5 Performance Characteristics of Elementary Sorts ' 267
6.6 Shellsort ' 273
6.7 Sorting Other Types of Data, 281
6.8 Index and Pointer Sorting, 287
6.9 Sorting of Linked Lists ' 294
6,10 Key-Indexed Counting, 298
Chapter 7. Quicksort 303
7.1 The Basic Algorithm. 304
7.2 Performance Characteristics of Quicksort, 309
7.3 Stack Size· 313
7.4 SmallSubfiles' 316
7.5 Median-of-Three Partitioning· 319
7.6 Duplicate Keys· 324
7.7 Strings and Vectors· 327
7.8 Selection· 329
Chapter 8. Merging and Mergesort 335
8.1 Two-Way Merging· 336
8.2 Abstract In-place Merge, 339
8.3 Top-Down Mergesort . 341
8.4 Improvements to the Basic Algorithm, 344
8.5 Bottom-Up Mergesort . 347
8.6 Performance Characteristics of Mergesort . 351
8.7 Linked-List Implementations of Mergesort . 354
8.8 Recursion Revisited, 357
Chapter 9. Priority Queues and Heapsort 361
9.1 Elementary Implementations· 365
9.2 Heap Data Structure, 368
9.3 Algorithms on Heaps· 371
9.4 Heapsort . 376
9.5 Priority-Queue ADT . 383
9.6 Priority Queues for Index Items· 389
9.7 Binomial Queues· 392
Chapter 10. Radix Sorting 403
10.1 Bits, Bytes, and Words· 405
10.2 Binary Quicksort· 409
10.3 MSD Radix Sort· 413
10.4 Three-Way Radix Quicksort· 421
10.5 LSD Radix Sort· 425
10.6 Performance Characteristics of Radix Sorts· 428
10.7 Sublinear-Time Sorts· 433
Chapter 11. Special-Purpose Sorts 439
11.1 Batcher's Odd-Even Mergesort . 441
11.2 Sorting Networks . 446
11.3 External Sorting . 454
11.4 Sort-Merge Implementations· 460
11.5 Parallel Sort/Merge . 466
Chapter 12. Symbol Tables and BSTs
12.1 Symbol-Table Abstract Data Type· 479
12.2 Key-Indexed Search· 485
12.3 Sequential Search· 489
12.4 Binary Search· 497
12.5 Binary Search Trees (BSTs) . 502
12.6 Performance Characteristics of BSTs . 508
12.7 Index Implementations with Symbol Tables· 511
12.8 Insertion at the Root in BSTs . 516
12.9 BST Implementations of Other ADT Functions· 519
Chapter 13. Balanced Trees 529
13.1 Randomized BSTs . 533
13.2 Splay BSTs . 540
13.3 Top-Down 2-3-4 Trees· 546
13.4 Red-Black Trees· 5.51
13.5 Skip Lists . .561
13.6 Performance Characteristics· .569
Chapter 14. Hashing 573
14.1 Hash Functions . .574
14.2 Separate Chaining' .583
14.3 Linear Probing . S88
14.4 Double Hashing . 594
14.5 Dynamic Hash Tables· .599
14.6 Perspective· 603
Chapter 15. Radix Search 609
15.1 Digital Search Trees 610
15.2 Tries· 614
15.3 Patricia Tries· 623
15.4 Multiway Tries and TSTs . 632
15.5 Text String Index Algorithms· 648
Chapter 16. External Searching 655
16.1 Rules of the Game· 6.57
16.2 Indexed Sequential Access· 660
16.3 B Trees· 662
16.4 Extendible Hashing· 676
16.5 Perspective· 688

## Chapter 1. Introduction 3

Algorithms: methods for solving problems that are suited for computer implementation.

### 1.1 Algorithms . 4

Most algorithms of interest involve methods of organizing the data involved in the computation. Objects created in this way are called data structures, thus, algorithms and data structures go hand in hand.

### 1.2 A Sample Problem-Connectivity· 6

The connectivity problem is easily solved in terms of the find and union abstract operations.

### 1.3 Union-Find Algorithms . 11

The first step in the process of developing an efficient algorithm to solve a given problem is to implement a simple algorithm that solves the problem.

### 1.4 Perspective . 22

We follow the same basic steps to  consider various algorithms for fundamental problems:

* Decide on a complete and specific problem statement, including identifying fundamental abstract operations that are intrinsic to the problem.
* Carefully develop a succinct implementation for a straightforward algorithm.
* Develop improved implementations through a process of stepwise refinement, validating the efficacy of ideas for improvement through empirical analysis, mathematical analysis, or both.
* Find high-level abstract representations of data structures or algorithms in operation that enable effective high-level design of improved versions.
* Strive for worst-case performance guarantees when possible, but accept good performance on actual data when available.

### 1.5 Summary of Topics· 23

Fundamentals (Part I) in the context of this book are the basic principles and methodology that we use to implement, analyze, and compare algorithms.

Data Structures (Part 2) go hand-in-hand with algorithms: we shall develop a thorough understanding of data representation methods for use throughout the rest of the book.

Sorting algorithms (Part 3) for rearranging files into order are of fundamental importance.

Searching algorithms (Part 4) for finding specific items among large collections of items are also of fundamental importance.

## Chapter 2. Principles of Algorithm Analysis 27

Analysis plays a role at every point in the process of designing and implementing algorithms.

• Illustrate the process.
• Describe in one place the mathematical conventions that we use.
• Provide a basis for discussion of higher-level issues.
• Develop an appreciation for scientific underpinnings of the conclusions that we draw when comparing algorithms.

### 2.1 Implementation and Empirical Analysis· 28

One of the first steps that we take to understand the performance of algorithms is to do empirical analysis.

The first challenge that we face in empirical analysis is to develop a correct and complete implementation.

The second challenge that we face in empirical analysis is to determine the nature of the input data and other factors that have direct influence on the experiments to be performed.

Perhaps the most common mistake made in selecting an algorithm is to ignore performance characteristics.

Perhaps the second most common mistake made in selecting an algorithm is to pay too much attention to performance characteristics.

### 2.2 Analysis of Algorithms . 33

The first step in the analysis of an algorithm is to identify the abstract operations on which the algorithm is based, to separate the analysis from the implementation.

We also have to study the data, and to model the input that might be presented to the algorithm. Most often, we consider one of two approaches to the analysis: we either assume that the input is random, and study the average-case performance of the program, or we look for perverse input, and study the worst-case performance of the program.

### 2.3 Growth of Functions . 36

Most algorithms have a primary parameter N that affects the running time most significantly.

The algorithms in this book typically have running times proportional to one of the following functions:
1, log N, N, N*log N ,N^2, N^3, 2^N。
lgN is binary logarithm, lnN is natural logarithm.

### 2.4 Big-Oh notation· 44

The mathematical artifact that allows us to suppress detail when we are analyzing algorithms is called the O-notation, or "big-Oh notation," which is defined as follows.

A function g(N) is said to be O(f(N)) if there exist constants Co and No such that g(N) < cof(N) for all N > No.

We use O-notation primarily to learn the fundamental asymptotic behavior of an algorithm; is proportional to when we want to predict performance by extrapolation from empirical studies; and about when we want to compare performance or to make absolute performance predictions.

### 2.5 Basic Recurrences· 49

A great many algorithms are based on the principle of recursively decomposing a large problem into one or more smaller ones, using solutions to the subproblems to solve the original problem.

### 2.6 Examples of Algorithm Analysis· 53

We use mathematical analysis of the frequency with which algorithms perform critical abstract operations, then use those results to deduce the functional form of the running time, which allows us to verify and extend empirical studies.

### 2.7 Guarantees, Predictions, and Limitations . 59

In this book, we take the view that algorithm design, careful implementation, mathematical analysis, theoretical studies, and empirical analysis all contribute in important ways to the development of elegant and efficient programs.

## Chapter 3. Elementary Data Structures 69

For many applications, the choice of the proper data structure is the only major decision involved in the implementation: once the choice has been made, the necessary algorithms are simple.

### 3.1 Building Blocks· 70

In C, our programs are all built from just a few basic types of data:
• Integers (ints).
• Floating-point numbers (floats).
• Characters (chars).

A data type is a set of values and a collection of operations on those values.

### 3.2 Arrays· 82

An array is a fixed collection of same-type data that are stored contiguously and that are accessible by an index.

### 3.3 Linked Lists· 90

A linked list is a set of items where each item is part of a node that also contains a link to a node.

• It is a null link that points to no node.
• It refers to a dummy node that contains no item.
• It refers back to the first node, making the list a circular list.

### 3.4 Elementary List Processing . 96

A linked list is either a null link or a link to a node that contains an item and a link to a linked list.

Working with data that are organized in linked lists is called list processing.

### 3.5 Memory Allocation for Lists . 105

An advantage of linked lists over arrays is that linked lists gracefully grow and shrink during their lifetime.

### 3.6 Strings . 109

We use the term string to refer to a variable-length array of characters, defined by a starting point and by a string-termination character marking the end.

### 3.7 Compound Data Structures· 115

A graph is a fundamental combinatorial object that is defined simply as a set of objects (called vertices) and a set of connections among the vertices (called edges).

## Chapter 4. Abstract Data Types 127

An abstract data type (ADT) is a data type (a set of values and a collection of operations on those values) that is accessed only through an interface. We refer to a program that uses an ADT as a client, and a program that specifies the data type as an implementation.

### 4.1 Abstract Objects and Collections of Objects· 131

Data types comprising collections of abstract objects (generalized queues) are a central object of study in computer science because they directly support a fundamental paradigm of computation.

### 4.2 Pushdown Stack ADT . 135

A pushdown stack is an ADT that comprises two basic operations: insert (push) a new item, and delete (pop) the item that was most recently inserted.

### 4.3 Examples of Stack ADT Clients . 138

We begin by considering a simpler problem, where the expression that we need to evaluate is in a form where each operator appears after its two arguments, rather than between them.

### 4.4 Stack ADT Implementations· 144

In this section, we consider two implementations of the stack ADT: one using arrays and one using linked lists.

### 4.5 Creation of a New ADT . 149

Splitting the program into three parts is a more effective approach because it
•   Separates the task of solving the high-level (connectivity) problem from the task of solving the low-level (union-find) problem, allowing us to work on the two problems independently
•   Gives us a natural way to compare different algorithms and data structures for solving the problem
•    Gives us an abstraction that we can use to build other algorithms
•   Defines, through the interface, a way to check that the software is operating as expected
•    Provides a mechanism that allows us to upgrade to new representations (new data structures or new algorithms) without changing the client program at all

### 4.6 FIFO Queues and Generalized Queues . 153

A FIFO queue is an ADT that comprises two basic operations: insert (put) a new item, and delete (get) the item that was least recently inserted.

We can implement the get and put operations for the FIFO queue ADT in constant time, using either arrays or linked lists.

### 4.7 Duplicate and Index Items· 161

For many applications, the abstract items that we process are unique, a quality that leads us to consider modifying our idea of how stacks, FIFO queues, and other generalized ADTs should operate.

### 4.8 First-Class ADTs . 165

A first-class data type is one for which we can have potentially many different instances, and which we can assign to variables which we can declare to hold the instances.

### 4.9 Application-Based ADT Example . 178

As a final example, we consider in this section an application-specific ADT that is representative of the relationship between applications domains and the algorithms and data structures of the type that we consider in this book.

### 4.10 Perspective· 184

•   ADTs are an important software-engineering tool in widespread use, and many of the algorithms that we study serve as implementations for fundamental ADTs that are widely applicable.
•    ADTs help us to encapsulate the algorithms that we develop, so that we can use the same code for many different purposes.
• ADTs provide a convenient mechanism for our use in the process of developing and comparing the performance of algorithms.

## Chapter 5. Recursion and Trees 187

A recursive program in a programming language is one that calls itself and  there must be a termination condition when the program can cease to call itself.

The study of recursion is intertwined with the study of recursively defined structures known as trees.

### 5.1 Recursive Algorithms· 188

A recursive algorithm is one that solves a problem by solving one or more smaller instances of the same problem.

### 5.2 Divide and Conquer . 196

A recursive function that divides a problem of size N into two independent (nonempty) parts that it solves recursively calls itself less than N times.

### 5.3 Dynamic Programming· 208

Dynamic programming reduces the running time of a recursive function to be at most the time required to evaluate the function for all arguments less than or equal to the given argument, treating the cost ofa recursive call as constant.

### 5.4 Trees· 216

A tree is a nonempty collection of vertices and edges that satisfies certain requirements.

A vertex is a simple object (also referred to as a node) that can have a name and can carry other associated information; an edge is a connection between two vertices.

A path in a tree is a list of distinct vertices in which successive vertices are connected by edges in the tree.

The defining property of a tree is that there is precisely one path connecting any two nodes.

If there is more than one path between some pair of nodes, or if there is no path between some pair of nodes, then we have a graph; we do not have a tree. A disjoint set of trees is called a forest.

A rooted tree is one where we designate one node as the root of a tree.

That is, a binary tree is a special type of ordered tree, an ordered tree is a special type of rooted tree, and a rooted tree is a special type of free tree.

A binary tree is either an external node or an internal node connected to a pair of binary trees, which are called the left subtree and the right subtree of that node.

An M-ary tree is either an external node or an internal node connected to an ordered sequence of IvI trees that are also AI-ary trees.

A tree (also called an ordered tree) is a node (called the root) connected to a sequence of disjoint trees. Such a sequence is called a forest.

There is a one-to-one correspondence between binary trees and ordered forests.

A rooted tree (or unordered tree) is a node (called the root) connected to a multiset ofrooted trees. (Such a multiset is called an unordered forest.)

A graph is a set of nodes together with a set of edges that connect pairs of distinct nodes (with at most one edge connecting any pair of nodes).

We consider a graph to be a tree if it satisfies any of the following four conditions:
• G has N - 1 edges and no cycles.
• G has N - 1 edges and is connected.
• Exactly one simple path connects each pair of vertices in G.
• G is connected, but does not remain connected if any edge is removed.

### 5.5 Mathematical Properties of Trees· 226

A binary tree with N internal nodes has N +1 external nodes.

A binary tree with N internal nodes has 2N links: N -1 links to internal nodes and N + 1 links to external nodes.

The level of a node in a tree is one higher than the level of its parent (with the root at level 0). The height of a tree is the maximum ofthe levels ofthe tree's nodes. The path length ofa tree is the sum of the levels of all the tree's nodes. The internal path length of a binary tree is the sum of the levels ofall the tree's internal nodes.  The external path length ofa binary tree is the sum of the levels of all the tree's external nodes.

The external path length of any binary tree with N internal nodes is 2N greater than the internal path length.

The height of a binary tree with N internal nodes is at least 19 N and at most N - l.

The internal path length ofa binary tree with N internal nodes is at least N*lg(N/4) and at most N(N - 1)/2.

### 5.6 Tree Traversal· 230

For binary trees, we have two links, and we therefore have three basic orders in which we might visit the nodes:
•    Preorder, where we visit the node, then visit the left and right subtrees
•   Inorder, where we visit the left subtree, then visit the node, then visit the right subtree
•    Postorder, where we visit the left and right subtrees, then visit the node

### 5.7 Recursive Binary-Tree Algorithms· 235

Our goal is to build a tournament: a binary tree where the item in every internal node is a copy of the larger of the items in its two children. In particular, the item at the root is a copy of the largest item in the tournament. The items in the leaves (nodes with no children) constitute the data of interest, and the rest of the tree is a data structure that allows us to find the largest of the items efficiently.

### 5.8 Graph Traversal· 241

Depth-first search requires time proportional to V +E in a graph with V vertices and E edges, using the adjacency lists representation.

### 5.9 Perspective· 247

Recursion lies at the heart of early theoretical studies into the nature of computation. Recursive functions and programs playa central role in mathematical studies that attempt to separate problems that can be solved by a computer from problems that cannot be.

## Chapter 6. Elementary Sorting Methods 253

As a rule, the elementary methods that we discuss here take time proportional to N^2 to sort N randomly arranged items.

### 6.1 Rules of the Game· 255

We shall be considering methods of sorting files of items containing keys.The keys, which are only part (often a small part) of the items, are used to control the sort. The objective of the sorting method is to rearrange the items such that their keys are ordered according to some well-defined ordering rule (usually numerical or alphabetical order).

A sorting method is said to be stable if it preserves the relative order of items with duplicated keys in the file.

### 6.2 Selection Sort· 261

Selection sort works by repeatedly selecting the smallest remaining element.

### 6.3 Insertion Sort ' 262

The method that people often use to sort bridge hands is to consider the elements one at a time, inserting each into its proper place among those already considered (keeping them sorted).

### 6.4 Bubble Sort· 265

Keep passing through the file, exchanging adjacent elements that are out of order, continuing until the file is sorted.

### 6.5 Performance Characteristics of Elementary Sorts ' 267

Selection sort uses about N^2/2 comparisons and N exchanges.

Insertion sort uses about N^2/4 comparisons and N^2/4 half-exchanges (moves) on the average, and twice that many at worst.

Bubble sort uses about N^2/2 comparisons and N^2/2 exchanges on the average and in the worst case.

An inversion is a pair of keys that are out of order in the file.

Insertion sort and bubble sort use a linear number of comparisons and exchanges for files with at most a constant number of inversions corresponding to each element.

Insertion sort uses a linear number of comparisons and exchanges for files with at most a constant number of elements having more than a constant number of corresponding inversions.

Selection sort runs in linear time for files with large items and small keys.

### 6.6 Shellsort ' 273

The idea is to rearrange the file to give it the property that taking every hth element (starting anywhere) yields a sorted file. Such a file is said to be h-sorted.

Our primary purpose in doing so is to illustrate that even algorithms that are apparently simple can have complex properties, and that the analysis of algorithms is not just of practical importance but also can be intellectually challenging.

The result of h-sorting a file that is k-ordered is a file that is both h- and k-ordered.

Shellsort does less than N(h-1)(k-1)/g comparisons to g-sort a file that is h- and k-ordered, provided that hand k are relatively prime.

Shellsort does less than O(N^(3/2)) comparisons for the increments 1 4 13 40 121 364 1093 3280 9841 ....

Shellsort does less than O(N^(4/3)) comparisons for the
increments 1 8 23 77 281 1073 4193 16577 ....

### 6.7 Sorting Other Types of Data, 281

We consider implementations, interfaces, and client programs for:
• Items, or generic objects to be sorted
• Arrays of items
The item data type provides us with a way to use our sort code for any type of data for which certain basic operations are defined.

### 6.8 Index and Pointer Sorting, 287

One simple approach for sorting without (intermediate) moves of items is to maintain an index array with keys in the items accessed only for comparisons.

### 6.9 Sorting of Linked Lists ' 294

The possibility that nodes could be referenced through pointers that are maintained outside the sort means that our programs should change only links in nodes, and should not alter keys or other information.

### 6,10 Key-Indexed Counting, 298

Key-indexed counting is a linear-time sort, provided that the range of distinct key values is within a constant factor of the file size.

## Chapter 7. Quicksort 303

Quicksort is probably used more widely than any other algorithm.

The quicksort algorithm has the desirable features that it is inplace (uses only a small auxiliary stack), requires requires time only proportional to N log N on the average to sort N items, and has an extremely short inner loop.

For huge files, however, quicksort is likely to run five to ten times as fast as shell sort, and it can adapt to be even more efficient for other types of files that might occur in practice.

### 7.1 The Basic Algorithm. 304

The partitioning process rearranges the array to make the following three conditions hold:
• The element a [i] is in its final place in the array for some i.
• None of the elements in a [l], .. , , a [i-1] is greater than a [i] .
• None of the elements in a [i+1], ' .. , a [r] is less than a [i] .

### 7.2 Performance Characteristics of Quicksort, 309

Quicksort uses about N^2 /2 comparisons in the worst case.

Quicksort uses about 2NInN comparisons on the average.

### 7.3 Stack Size· 313

If the smaller of the two subfiles is sorted first, then the stack never has more than 19 N entries when quicksort is used to sort N elements.

### 7.4 SmallSubfiles' 316

A definite improvement to quicksort  is to change the test at the beginning of the recursive routine from a return to a call on insertion sort, as follows:
if (r-1 <= M) insertion(a, 1, r); // M = 9 or from 5 to 25.

### 7.5 Median-of-Three Partitioning· 319

Another well-known way to find a better partitioning element is to take a sample of three elements from the file, then to use the median of the three for the partitioning element.

### 7.6 Duplicate Keys· 324

One straightforward idea is to partition the file into three parts, one each for keys smaller than, equal to, and larger than the partitioning element。

### 7.7 Strings and Vectors· 327

When the sort keys are strings, we could use an abstract-string type implementation like Program 6. I I with the quicksort implementations in this chapter.

The three-way partitioning procedure that we considered in Section 7.6 provides an elegant way to take advantage of this observation.

### 7.8 Selection· 329

Quicksort-based selection is linear time on the average.

## Chapter 8. Merging and Mergesort 335

One of mergesort's most attractive properties is that it sorts a file of N elements in time proportional to N log N, no matter what the input.

### 8.1 Two-Way Merging· 336

Mergesort requires about N*lgN comparisons to sort any file of N elements.

### 8.2 Abstract In-place Merge, 339

A sequence of keys that increases, then decreases (or decreases, then increases) is referred to as a bitonic sequence.

### 8.3 Top-Down Mergesort . 341

Mergesort uses extra space proportional to N.

Mergesort is stable, if the underlying merge is stable.

The resource requirements ofmergesort are insensitive to the initial order of its input.

### 8.4 Improvements to the Basic Algorithm, 344

This technique eliminates the array copy at the expense of putting back into the inner loop the tests for whether the input arrays are exhausted.

### 8.5 Bottom-Up Mergesort . 347

All the merges in each pass of a bottom-up mergesort involve file sizes that are a power of 2, except possibly the final file size.

The number of passes in a bottom-up mergesort of N elements is precisely the number of bits in the binary representation of N (ignoring leading 0 bits).

### 8.6 Performance Characteristics of Mergesort . 351

The allure of premature optimization is so strong that it is worthwhile to reinforce them each time that we study techniques for performance improvement at this level of detail.

### 8.7 Linked-List Implementations of Mergesort . 354

In fact, mergesort turns out to be well-suited to linked lists.

### 8.8 Recursion Revisited, 357

Quicksort might perhaps more properly be called a conquer-anddivide algorithm: In a recursive implementation, most of the work for a particular activation is done before the recursive calls. On the other hand, the recursive merge sort has more the spirit of divide and conquer: First, the file is divided into two parts; then, each part is conquered individually.

## Chapter 9. Priority Queues and Heapsort 361

A priority queue is a data structure of items with keys that supports two basic operations: insert a new item, and delete the item with the largest key.

We want to build and maintain a data structure containing records with numerical keys (priorities) that supports some of the following operations:
• Construct a priority queue from N given items.
• Insert a new item.
• Delete the maximum item.
• Change the priority of an arbitrary specified item.
• Delete an arbitrary specified item.
• Join two priority queues into one large one.

### 9.1 Elementary Implementations· 365

It is wise to keep in mind the simple implementations because they often can outperform more complicated methods in many practi­cal situations.

### 9.2 Heap Data Structure, 368

A tree is heap-ordered if the key in each node is larger than or equal to the keys in all of that node's children (if any).  Equivalently, the key in each node of a heap-ordered tree is smaller than or equal to the key in that node's parent (if any).

No node in a heap-ordered tree has a key larger than the key at the root.

A heap is a set of nodes with keys arranged in a complete heap-ordered binary tree, represented as an array. 

### 9.3 Algorithms on Heaps· 371

The priority-queue algorithms on heaps all work by first making a simple modification that could violate the heap condition, then traveling through the heap, modifying the heap as required to ensure that the heap condition is satisfied everywhere. This process is sometimes called heapifying, or just fixing the heap.

The insert and delete the maximum operations for the priority queue abstract data type can be implemented with heapordered trees such that insert requires no more than lgN comparisons and delete the maximum no more than 2lg N comparisons, when performed on an N -item queue.

The change priority, delete, and replace the maximum operations for the priority queue abstract date type can be implemented with the heap-ordered trees such that no more than 2lgN comparisons are required for any operation on an N-item queue.

### 9.4 Heapsort . 376

Bottom-up heap construction takes linear time. 

Heapsort uses fewer than 2N*lgN comparisons to sort N elements. 

Heap-based selection allows the kth largest of N items to be found in time proportional to N when k is small or close to N, and in time proportional to N log N otherwise.

### 9.5 Priority-Queue ADT . 383

In applications where the priority queue does not grow to be large, or where the mix of insert and delete the maximum operations has some special properties, a fully flexible interface might be desirable. On the other hand, in applications where the queue will grow to be large, and where a tenfold or a hundredfold increase in performance might be noticed or appreciated, it might be worthwhile to restrict to the set of operations where efficient performance is assured.

### 9.6 Priority Queues for Index Items· 389

Suppose that the records to be processed in a priority queue are in an existing array. In this case, it makes sense to have the priority-queue routines refer to items through the array index. Moreover, we can use the array index as a handle to implement all the priority-queue operations.

### 9.7 Binomial Queues· 392

A binary tree comprising nodes with keys is said to be left heap ordered if the key in each node is larger than or equal to all the keys in that node's left subtree (if any).

A power-of-2 heap is a left-heap-ordered tree consisting of a root node with an empty right subtree and a complete left subtree. The tree corresponding to a power-of-2 heap by the left-child, right-sibling correspondence is called a binomial tree.

A binomial queue is a set of power-of-2 heaps, no two of the same size. The structure of a binomial queue is determined by that queue's number of nodes, by correspondence with the binary representation of integers.

All the operations for the priority-queue ADT can be implemented with binomial queues such that O(lg N) steps are required for any operations performed on an N -item queue.

Construction of a binomial queue with N insert operations on an initially empty queue requires O(N) comparisons in the worst case.

## Chapter 10. Radix Sorting 403

Sorting methods built on processing numbers one piece at a time are called radix sorts.

Radix-sorting algorithms are based on the abstract operation "extract theith digit from a key."

There are two, fundamentally different, basic approaches to radix sorting. The first class of methods involves algorithms that examine the digits in the keys in a left-to-right order, working with the most significant digits first. These methods are generally referred to as most-significant-digit (MSD) radix sorts. 

The second class of radix-sorting methods is different: They examine the digits in the keys in a right-to-left order, working with the least significant digits first. These methods are generally referred to as least-significant-digit (LSD) radix sorts.

### 10.1 Bits, Bytes, and Words· 405

The key to understanding radix sort is to recognize that (i) computers generally are built to process bits in groups called machine words, which are often grouped into smaller pieces call bytes; (ii) sort keys also are commonly organized as byte sequences; and (iii) small byte sequences can also serve as array indices or machine addresses. Therefore, it will be convenient for us to work with the following abstractions.

A byte is a fixed-length sequence of bits; a string is a variable-length sequence of bytes; a word is a fixed-length sequence of bytes.

A key is a radix-R number, with digits numbered from the left (starting at 0).

### 10.2 Binary Quicksort· 409

Radix sorting simply does not work well on files that contain huge numbers of duplicate keys that are not short.

### 10.3 MSD Radix Sort· 413

Using just 1 bit in radix quicksort amounts to treating keys as radix2 (binary) numbers and considering the most significant digits first. 

Generalizing, suppose that we wish to sort radix-R numbers by con­sidering the most significant bytes first. Doing so requires partitioning the array into R, rather than just two, different parts.

### 10.4 Three-Way Radix Quicksort· 421

In essence, doing three-way radix quicksort amounts to sorting the file on the leading characters of the keys (using quicksort), then applying the method recursively on the remainder of the keys. For sorting strings, the method compares favorably with normal quicksort and with MSD radix sort. Indeed, it might be viewed as a hybrid of these two algorithms.

### 10.5 LSD Radix Sort· 425

An alternative radix-sorting method is to examine the bytes from right to left. 

### 10.6 Performance Characteristics of Radix Sorts· 428

The running time of LSD radix sort for sorting N records with w-byte keys is proportional to N*w, because the algorithm makes ill passes over all N keys.

The worst case for radix sorting is to examine all the bytes in all the keys.

Binary quicksort examines about N*lgN bits, on average, when sorting keys composed of random bits. 

MSD radix sort with radix R on a file of size N requires at least 2N + 2R steps.

If the radix is always less than the file size, the number of steps taken by MSD radix sort is within a small constant factor of N logR N on the average (for keys comprising random bytes), and within a small constant factor ofthe number of bytes in the keys in the worst case.

Three-way radix quicksort uses 2N*lnN byte comparisons, on the average, to sort N (arbitrarily long) keys. 

LSD radix sort can sort N records with w-bit keys in w/lgR passes, using extra space for R counters (and a buffer for rearranging the file).

### 10.7 Sublinear-Time Sorts· 433

The LSD approach to radix sorting is widely used, because it involves extremely simple control structures and its basic operations are suitable for machine-language implementation, which can directly adapt to special-purpose high-performance hardware.

## Chapter 11. Special-Purpose Sorts 439

In this chapter, we examine examples of sorting methods that are designed to run efficiently on various different kinds of machines. We consider several different examples of the restrictions imposed by high-performance hardware, and several methods that are useful in practice for implementing high-performance sorts.

### 11.1 Batcher's Odd-Even Mergesort . 441

A nonadaptive sorting algorithm is one where the sequence of operations performed depends on only the number of the inputs, rather than on the values of the keys.

If a nonadaptive program produces sorted output when the inputs are all either 0 or 1, then it does so when the inputs are arbitrary keys.

dcba cdab abcd abcd
dbca bdac acbd abcd
12-34, 13-24, 23

### 11.2 Sorting Networks . 446

Batcher's odd-even sorting networks have about N (lg N)^2 / 4 comparators and can run in (lg N)^2 /2 parallel steps.

### 11.3 External Sorting . 454

In summary, the abstract model that we shall use for external sorting involves a basic assumption that the file to be sorted is far too large to fit in main memory, and accounts for two other resources: running time (number of passes through the data) and the number of external devices available for use.

With 2P external devices and internal memory sufficient to hold M records, a sort-merge that is based on a P-way balanced merge takes about 1 + logp(N/M) passes.

For random keys, the runs produced by replacement selection are about twice the size ofthe heap used.

### 11.4 Sort-Merge Implementations· 460

For random keys, the runs produced by replacement selection are about twice the size ofthe heap used.

With three external devices and internal memory sufficient to hold A1 records, a sort-merge that is based on replacement selection fol/owed by a two-way polyphase merge takes about 1 + log¢(N/2M)/¢ effective passes, on the average.

### 11.5 Parallel Sort/Merge . 466

A merging comparator takes as input two sorted files of size M, and produces as output two sorted files: one containing the M smallest of the 2M inputs, and the other containing the M largest of the 2M inputs.

We can sort a file of size N by dividing it into blocks ofsize M, sorting each file, then using a sorting network built with merging comparators.

Block sorting on P processors, using Batcher's sort with merging comparators, can sort N records in about (lgP)^2/2 parallel steps.

## Chapter 12. Symbol Tables and BSTs 477

THE RETRIEVAL OF a particular piece or pieces of information from large volumes of previously stored data is a fundamental operation, called search, that is intrinsic to a great many computational tasks.

A symbol table is a data structure ofitems with keys that supports two basic operations: insert a new item, and return an item with a given key.

### 12.1 Symbol-Table Abstract Data Type· 479

The operations of interest include
• Insert a new item.
• Search for an item (or items) having a given key.
• Delete a specified item.
• Select the kth smallest item in a symbol table.
• Sort the symbol table (visit all the items in order of their keys).
• Join two symbol tables. 

### 12.2 Key-Indexed Search· 485

If key values are positive integers less than M and items have distinct keys, then the symbol-table data type can be implemented with key-indexed arrays of items such that insert, search, and delete require constant time; and initialize, select, and sort require time proportional to M, whenever any of the operations are performed on an N -item table.

### 12.3 Sequential Search· 489

For general key values from too large a range for them to be used as indices, one simple approach for a symbol-table implementation is to store the items contiguously in an array, in order.

Sequential search in a symbol table with N items uses about N /2 comparisons for search hits (on the average).

Sequential search in a symbol table of N unordered items uses a constant number of steps for inserts and N comparisons for search misses (always).

Sequential search in a symbol table of N ordered items uses about N /2 comparisons for insertion, search hits, and search misses (on the average).

### 12.4 Binary Search· 497

Binary search never uses more than lgN+ 1 comparisons for a search (hit or miss). 

### 12.5 Binary Search Trees (BSTs) . 502

A binary search tree (BST) is a binary tree that has a key associated with each of its internal nodes, with the additional property that the key in any node is larger than (or equal to) the keys in all nodes in that node's left subtree and smaller than (or equal to) the keys in alJ nodes in that node's right subtree.

### 12.6 Performance Characteristics of BSTs . 508

Search hits require about 2lnN ~ 1.39lgN comparisons, on the average, in a BST built from N random keys. 

Insertions and search misses require about 2lnN ~ 1.39lgN comparisons, on the average, in a BST built from N random keys.

In the worst case, a search in a binary search tree with N keys can require N comparisons.

### 12.7 Index Implementations with Symbol Tables· 511

For many applications we want a search structure simply to help us find items, without moving them around. For example, we might have an array of items with keys, and we might want the search method to give us the index into that array of the item matching a certain key.

### 12.8 Insertion at the Root in BSTs . 516

A rotation is a local change, involving only three links and two nodes, that allows us to move nodes around in trees without changing the global ordering properties that make BSTs useful for search.

The rotation operations provide a straightforward recursive implementation of root insertion: Recursively insert the new item into the appropriate subtree (leaving it, when the recursive operation is complete, at the root of that tree), then rotate to make it the root of the main tree.

### 12.9 BST Implementations of Other ADT Functions· 519

In this section, we consider implementations of select, join, and delete. 

## Chapter 13. Balanced Trees 529

A randomized algorithm introduces random decision making into the algorithm itself, to reduce dramatically the chance of a worstcase scenario (no matter what the input).

An amortization approach is to do extra work at one time to avoid more work later, to be able to provide guaranteed upper bounds on the average per-operation cost (the total cost of all operations divided by the number of operations).

An optimization approach is to take the trouble to provide performance guarantees for every operation.

### 13.1 Randomized BSTs . 533

Building a randomized BST is equivalent to building a standard BST from a random initial permutation ofthe keys. We use about 2N*lnN comparisons to construct a randomized BST with N items (no matter in what order the items are presented for insertion), and about 2lnN comparisons for searches in such a tree.

The probability that the construction cost of a randomized BST is more than a factor of a times the average is less than e^-a:

Making a tree with an arbitrary sequence of randomized insert, delete, and join operations is equivalent to building a standard BST from a random permutation of the keys in the tree.

### 13.2 Splay BSTs . 540

The number of comparisons used when a splay BST is built from N insertions into an initially empty tree is O(N*lgN).

The number of comparisons required for any sequence of M insert or search operations in an N -node splay BST is O((N + M)lg(N + M)).

### 13.3 Top-Down 2-3-4 Trees· 546

A 2-3-4 search tree is a tree that either is empty or comprises three types of nodes: 2-nodes, with one key, a left link to a tree with smaller keys, and a right link to a tree with larger keys; 3-nodes, with two keys, a left link to a tree with smaller keys, a middle link to a tree with key values between the node's keys and a right link to a tree with larger keys; and 4-nodes, with three keys and four links to trees with key values defined by the ranges subtended by the node's keys.

A balanced 2-3-4 search tree is a 2-3-4 search tree with all links to empty trees at the same distance from the root.

Searches in N-node 2-3-4 trees uisit at most lgN + 1 nodes.

Insertions into N-node 2-3-4 trees require fewer than lgN+1 node splits in the worst case, and seem to require less than one node split on the average.

### 13.4 Red-Black Trees· 5.51

A search in a red-black tree with N nodes requires fewer than 2lgN + 2 comparisons.

A search in a red-black tree with N nodes built from random keys uses about 1.002lg N comparisons, on the average.

A red-black BST is a binary search tree in which each node is marked to be either red or black, with the additional restriction that no two red nodes appear consecutively on any path from an external link to the root.

A balanced red-black BST is a red-black BST in which all paths from external links to the root have the same number of black nodes.

### 13.5 Skip Lists . .561

A skip list is an ordered linked list where each node contains a variable number of links, with the ith links in the nodes implementing singly linked lists that skip the nodes with fewer than i links.

Search and insertion in a randomized skip list with parameter t require about (t logt N)/2 = (t/(2lgt))lg N comparisons, on the average.

Skip lists have (t / (t - 1))N links on the average. 

### 13.6 Performance Characteristics· .569

Splay BSTs will provide good performance as a self-organizing search method, particularly when frequent access to a small set of keys is a typical pattern; randomized BSTs are likely to be faster and easier to implement for a full-function symbol table BST; skip lists are easy to understand and can provide logarithmic search with less space than the other methods, and red-black BSTs are attractive for symbol-table library implementations, because they provide guaranteed performance bounds in the worst case and the fastest search and insertion algorithms for random data.

## Chapter 14. Hashing 573

### 14.1 Hash Functions . .574

### 14.2 Separate Chaining' .583

### 14.3 Linear Probing . S88

### 14.4 Double Hashing . 594

### 14.5 Dynamic Hash Tables· .599

### 14.6 Perspective· 603

## Chapter 15. Radix Search 609

### 15.1 Digital Search Trees 610

### 15.2 Tries· 614

### 15.3 Patricia Tries· 623

### 15.4 Multiway Tries and TSTs . 632

### 15.5 Text String Index Algorithms· 648

## Chapter 16. External Searching 655

### 16.1 Rules of the Game· 6.57

### 16.2 Indexed Sequential Access· 660

### 16.3 B Trees· 662

### 16.4 Extendible Hashing· 676

### 16.5 Perspective· 688

