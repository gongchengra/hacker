# Algorithms in C

## contents
Chapter 1. Introduction 3
1.1 Algorithms . 4
1.2 A Sample Problem-Connectivity· 6
1.3 Union-Find Algorithms . 11
1.4 Perspective . 22
1.5 Summary of Topics· 23
Chapter 2. Principles of Algorithm Analysis 27
2.1 Implementation and Empirical Analysis· 28
2.2 Analysis of Algorithms . 33
2.3 Growth of Functions . 36
2.4 Big-Oh notation· 44
2.5 Basic Recurrences· 49
2.6 Examples of Algorithm Analysis· 53
2.7 Guarantees, Predictions, and Limitations . 59
Chapter 3. Elementary Data Structures 69
3.1 Building Blocks· 70
3.2 Arrays· 82
3.3 Linked Lists· 90
3.4 Elementary List Processing . 96
3.5 Memory Allocation for Lists . 105
3.6 Strings . 109
3.7 Compound Data Structures· 115
Chapter 4. Abstract Data Types 127
4.1 Abstract Objects and Collections of Objects· 131
4.2 Pushdown Stack ADT . 135
4.3 Examples of Stack ADT Clients . 138
4.4 Stack ADT Implementations· 144
4.5 Creation of a New ADT . 149
4.6 FIFO Queues and Generalized Queues . 153
4.7 Duplicate and Index Items· 161
4.8 First-Class ADTs . 165
4.9 Application-Based ADT Example . 178
4.10 Perspective· 184
Chapter 5. Recursion and Trees 187
5.1 Recursive Algorithms· 188
5.2 Divide and Conquer . 196
5.3 Dynamic Programming· 208
5.4 Trees· 216
5.5 Mathematical Properties of Trees· 226
5.6 Tree Traversal· 230
5.7 Recursive Binary-Tree Algorithms· 235
5.8 Graph Traversal· 241
5.9 Perspective· 247
Chapter 6. Elementary Sorting Methods 253
6.1 Rules of the Game· 255
6.2 Selection Sort· 261
6.3 Insertion Sort ' 262
6.4 Bubble Sort· 265
6.5 Performance Characteristics of Elementary Sorts ' 267
6.6 Shellsort ' 273
6.7 Sorting Other Types of Data, 281
6.8 Index and Pointer Sorting, 287
6.9 Sorting of Linked Lists ' 294
6,10 Key-Indexed Counting, 298
Chapter 7. Quicksort 303
7.1 The Basic Algorithm. 304
7.2 Performance Characteristics of Quicksort, 309
7.3 Stack Size· 313
7.4 SmallSubfiles' 316
7.5 Median-of-Three Partitioning· 319
7.6 Duplicate Keys· 324
7.7 Strings and Vectors· 327
7.8 Selection· 329
Chapter 8. Merging and Mergesort 335
8.1 Two-Way Merging· 336
8.2 Abstract In-place Merge, 339
8.3 Top-Down Mergesort . 341
8.4 Improvements to the Basic Algorithm, 344
8.5 Bottom-Up Mergesort . 347
8.6 Performance Characteristics of Mergesort . 351
8.7 Linked-List Implementations of Mergesort . 354
8.8 Recursion Revisited, 357
Chapter 9. Priority Queues and Heapsort 361
9.1 Elementary Implementations· 365
9.2 Heap Data Structure, 368
9.3 Algorithms on Heaps· 371
9.4 Heapsort . 376
9.5 Priority-Queue ADT . 383
9.6 Priority Queues for Index Items· 389
9.7 Binomial Queues· 392
Chapter 10. Radix Sorting 403
10.1 Bits, Bytes, and Words· 405
10.2 Binary Quicksort· 409
10.3 MSD Radix Sort· 413
10.4 Three-Way Radix Quicksort· 421
10.5 LSD Radix Sort· 425
10.6 Performance Characteristics of Radix Sorts· 428
10.7 Sublinear-Time Sorts· 433
Chapter 11. Special-Purpose Sorts 439
11.1 Batcher's Odd-Even Mergesort . 441
11.2 Sorting Networks . 446
11.3 External Sorting . 454
11.4 Sort-Merge Implementations· 460
11.5 Parallel Sort/Merge . 466
Chapter 12. Symbol Tables and BSTs
12.1 Symbol-Table Abstract Data Type· 479
12.2 Key-Indexed Search· 485
12.3 Sequential Search· 489
12.4 Binary Search· 497
12.5 Binary Search Trees (BSTs) . 502
12.6 Performance Characteristics of BSTs . 508
12.7 Index Implementations with Symbol Tables· 511
12.8 Insertion at the Root in BSTs . 516
12.9 BST Implementations of Other ADT Functions· 519
Chapter 13. Balanced Trees 529
13.1 Randomized BSTs . 533
13.2 Splay BSTs . 540
13.3 Top-Down 2-3-4 Trees· 546
13.4 Red-Black Trees· 5.51
13.5 Skip Lists . .561
13.6 Performance Characteristics· .569
Chapter 14. Hashing 573
14.1 Hash Functions . .574
14.2 Separate Chaining' .583
14.3 Linear Probing . S88
14.4 Double Hashing . 594
14.5 Dynamic Hash Tables· .599
14.6 Perspective· 603
Chapter 15. Radix Search 609
15.1 Digital Search Trees 610
15.2 Tries· 614
15.3 Patricia Tries· 623
15.4 Multiway Tries and TSTs . 632
15.5 Text String Index Algorithms· 648
Chapter 16. External Searching 655
16.1 Rules of the Game· 6.57
16.2 Indexed Sequential Access· 660
16.3 B Trees· 662
16.4 Extendible Hashing· 676
16.5 Perspective· 688

## Chapter 1. Introduction 3

Algorithms: methods for solving problems that are suited for computer implementation.

### 1.1 Algorithms . 4

Most algorithms of interest involve methods of organizing the data involved in the computation. Objects created in this way are called data structures, thus, algorithms and data structures go hand in hand.

### 1.2 A Sample Problem-Connectivity· 6

The connectivity problem is easily solved in terms of the find and union abstract operations.

### 1.3 Union-Find Algorithms . 11

The first step in the process of developing an efficient algorithm to solve a given problem is to implement a simple algorithm that solves the problem.

### 1.4 Perspective . 22

We follow the same basic steps to  consider various algorithms for fundamental problems:

* Decide on a complete and specific problem statement, including identifying fundamental abstract operations that are intrinsic to the problem.
* Carefully develop a succinct implementation for a straightforward algorithm.
* Develop improved implementations through a process of stepwise refinement, validating the efficacy of ideas for improvement through empirical analysis, mathematical analysis, or both.
* Find high-level abstract representations of data structures or algorithms in operation that enable effective high-level design of improved versions.
* Strive for worst-case performance guarantees when possible, but accept good performance on actual data when available.

### 1.5 Summary of Topics· 23

Fundamentals (Part I) in the context of this book are the basic principles and methodology that we use to implement, analyze, and compare algorithms.

Data Structures (Part 2) go hand-in-hand with algorithms: we shall develop a thorough understanding of data representation methods for use throughout the rest of the book.

Sorting algorithms (Part 3) for rearranging files into order are of fundamental importance.

Searching algorithms (Part 4) for finding specific items among large collections of items are also of fundamental importance.

## Chapter 2. Principles of Algorithm Analysis 27

Analysis plays a role at every point in the process of designing and implementing algorithms.

• Illustrate the process.
• Describe in one place the mathematical conventions that we use.
• Provide a basis for discussion of higher-level issues.
• Develop an appreciation for scientific underpinnings of the conclusions that we draw when comparing algorithms.

### 2.1 Implementation and Empirical Analysis· 28

One of the first steps that we take to understand the performance of algorithms is to do empirical analysis.

The first challenge that we face in empirical analysis is to develop a correct and complete implementation.

The second challenge that we face in empirical analysis is to determine the nature of the input data and other factors that have direct influence on the experiments to be performed.

Perhaps the most common mistake made in selecting an algorithm is to ignore performance characteristics.

Perhaps the second most common mistake made in selecting an algorithm is to pay too much attention to performance characteristics.

### 2.2 Analysis of Algorithms . 33

The first step in the analysis of an algorithm is to identify the abstract operations on which the algorithm is based, to separate the analysis from the implementation.

We also have to study the data, and to model the input that might be presented to the algorithm. Most often, we consider one of two approaches to the analysis: we either assume that the input is random, and study the average-case performance of the program, or we look for perverse input, and study the worst-case performance of the program.

### 2.3 Growth of Functions . 36

Most algorithms have a primary parameter N that affects the running time most significantly.

The algorithms in this book typically have running times proportional to one of the following functions:
1, log N, N, N*log N ,N^2, N^3, 2^N。
lgN is binary logarithm, lnN is natural logarithm.

### 2.4 Big-Oh notation· 44

The mathematical artifact that allows us to suppress detail when we are analyzing algorithms is called the O-notation, or "big-Oh notation," which is defined as follows.

A function g(N) is said to be O(f(N)) if there exist constants Co and No such that g(N) < cof(N) for all N > No.

We use O-notation primarily to learn the fundamental asymptotic behavior of an algorithm; is proportional to when we want to predict performance by extrapolation from empirical studies; and about when we want to compare performance or to make absolute performance predictions.

### 2.5 Basic Recurrences· 49

A great many algorithms are based on the principle of recursively decomposing a large problem into one or more smaller ones, using solutions to the subproblems to solve the original problem.

### 2.6 Examples of Algorithm Analysis· 53

We use mathematical analysis of the frequency with which algorithms perform critical abstract operations, then use those results to deduce the functional form of the running time, which allows us to verify and extend empirical studies.

### 2.7 Guarantees, Predictions, and Limitations . 59

In this book, we take the view that algorithm design, careful implementation, mathematical analysis, theoretical studies, and empirical analysis all contribute in important ways to the development of elegant and efficient programs.

## Chapter 3. Elementary Data Structures 69

For many applications, the choice of the proper data structure is the only major decision involved in the implementation: once the choice has been made, the necessary algorithms are simple.

### 3.1 Building Blocks· 70

In C, our programs are all built from just a few basic types of data:
• Integers (ints).
• Floating-point numbers (floats).
• Characters (chars).

A data type is a set of values and a collection of operations on those values.

### 3.2 Arrays· 82

An array is a fixed collection of same-type data that are stored contiguously and that are accessible by an index.

### 3.3 Linked Lists· 90

A linked list is a set of items where each item is part of a node that also contains a link to a node.

• It is a null link that points to no node.
• It refers to a dummy node that contains no item.
• It refers back to the first node, making the list a circular list.

### 3.4 Elementary List Processing . 96

A linked list is either a null link or a link to a node that contains an item and a link to a linked list.

Working with data that are organized in linked lists is called list processing.

### 3.5 Memory Allocation for Lists . 105

An advantage of linked lists over arrays is that linked lists gracefully grow and shrink during their lifetime.

### 3.6 Strings . 109

We use the term string to refer to a variable-length array of characters, defined by a starting point and by a string-termination character marking the end.

### 3.7 Compound Data Structures· 115

A graph is a fundamental combinatorial object that is defined simply as a set of objects (called vertices) and a set of connections among the vertices (called edges).

## Chapter 4. Abstract Data Types 127

An abstract data type (ADT) is a data type (a set of values and a collection of operations on those values) that is accessed only through an interface. We refer to a program that uses an ADT as a client, and a program that specifies the data type as an implementation.

### 4.1 Abstract Objects and Collections of Objects· 131

Data types comprising collections of abstract objects (generalized queues) are a central object of study in computer science because they directly support a fundamental paradigm of computation.

### 4.2 Pushdown Stack ADT . 135

A pushdown stack is an ADT that comprises two basic operations: insert (push) a new item, and delete (pop) the item that was most recently inserted.

### 4.3 Examples of Stack ADT Clients . 138

We begin by considering a simpler problem, where the expression that we need to evaluate is in a form where each operator appears after its two arguments, rather than between them.

### 4.4 Stack ADT Implementations· 144

In this section, we consider two implementations of the stack ADT: one using arrays and one using linked lists.

### 4.5 Creation of a New ADT . 149

Splitting the program into three parts is a more effective approach because it
•   Separates the task of solving the high-level (connectivity) problem from the task of solving the low-level (union-find) problem, allowing us to work on the two problems independently
•   Gives us a natural way to compare different algorithms and data structures for solving the problem
•    Gives us an abstraction that we can use to build other algorithms
•   Defines, through the interface, a way to check that the software is operating as expected
•    Provides a mechanism that allows us to upgrade to new representations (new data structures or new algorithms) without changing the client program at all

### 4.6 FIFO Queues and Generalized Queues . 153

A FIFO queue is an ADT that comprises two basic operations: insert (put) a new item, and delete (get) the item that was least recently inserted.

We can implement the get and put operations for the FIFO queue ADT in constant time, using either arrays or linked lists.

### 4.7 Duplicate and Index Items· 161

For many applications, the abstract items that we process are unique, a quality that leads us to consider modifying our idea of how stacks, FIFO queues, and other generalized ADTs should operate.

### 4.8 First-Class ADTs . 165

A first-class data type is one for which we can have potentially many different instances, and which we can assign to variables which we can declare to hold the instances.

### 4.9 Application-Based ADT Example . 178

As a final example, we consider in this section an application-specific ADT that is representative of the relationship between applications domains and the algorithms and data structures of the type that we consider in this book.

### 4.10 Perspective· 184

•   ADTs are an important software-engineering tool in widespread use, and many of the algorithms that we study serve as implementations for fundamental ADTs that are widely applicable.
•    ADTs help us to encapsulate the algorithms that we develop, so that we can use the same code for many different purposes.
• ADTs provide a convenient mechanism for our use in the process of developing and comparing the performance of algorithms.

## Chapter 5. Recursion and Trees 187

A recursive program in a programming language is one that calls itself and  there must be a termination condition when the program can cease to call itself.

The study of recursion is intertwined with the study of recursively defined structures known as trees.

### 5.1 Recursive Algorithms· 188

A recursive algorithm is one that solves a problem by solving one or more smaller instances of the same problem.

### 5.2 Divide and Conquer . 196

A recursive function that divides a problem of size N into two independent (nonempty) parts that it solves recursively calls itself less than N times.

### 5.3 Dynamic Programming· 208

Dynamic programming reduces the running time of a recursive function to be at most the time required to evaluate the function for all arguments less than or equal to the given argument, treating the cost ofa recursive call as constant.

### 5.4 Trees· 216

A tree is a nonempty collection of vertices and edges that satisfies certain requirements.

A vertex is a simple object (also referred to as a node) that can have a name and can carry other associated information; an edge is a connection between two vertices.

A path in a tree is a list of distinct vertices in which successive vertices are connected by edges in the tree.

The defining property of a tree is that there is precisely one path connecting any two nodes.

If there is more than one path between some pair of nodes, or if there is no path between some pair of nodes, then we have a graph; we do not have a tree. A disjoint set of trees is called a forest.

A rooted tree is one where we designate one node as the root of a tree.

That is, a binary tree is a special type of ordered tree, an ordered tree is a special type of rooted tree, and a rooted tree is a special type of free tree.

A binary tree is either an external node or an internal node connected to a pair of binary trees, which are called the left subtree and the right subtree of that node.

An M-ary tree is either an external node or an internal node connected to an ordered sequence of IvI trees that are also AI-ary trees.

A tree (also called an ordered tree) is a node (called the root) connected to a sequence of disjoint trees. Such a sequence is called a forest.

There is a one-to-one correspondence between binary trees and ordered forests.

A rooted tree (or unordered tree) is a node (called the root) connected to a multiset ofrooted trees. (Such a multiset is called an unordered forest.)

A graph is a set of nodes together with a set of edges that connect pairs of distinct nodes (with at most one edge connecting any pair of nodes).

We consider a graph to be a tree if it satisfies any of the following four conditions:
• G has N - 1 edges and no cycles.
• G has N - 1 edges and is connected.
• Exactly one simple path connects each pair of vertices in G.
• G is connected, but does not remain connected if any edge is removed.

### 5.5 Mathematical Properties of Trees· 226

A binary tree with N internal nodes has N +1 external nodes.

A binary tree with N internal nodes has 2N links: N -1 links to internal nodes and N + 1 links to external nodes.

The level of a node in a tree is one higher than the level of its parent (with the root at level 0). The height of a tree is the maximum ofthe levels ofthe tree's nodes. The path length ofa tree is the sum of the levels of all the tree's nodes. The internal path length of a binary tree is the sum of the levels ofall the tree's internal nodes.  The external path length ofa binary tree is the sum of the levels of all the tree's external nodes.

The external path length of any binary tree with N internal nodes is 2N greater than the internal path length.

The height of a binary tree with N internal nodes is at least 19 N and at most N - l.

The internal path length ofa binary tree with N internal nodes is at least N*lg(N/4) and at most N(N - 1)/2.

### 5.6 Tree Traversal· 230

For binary trees, we have two links, and we therefore have three basic orders in which we might visit the nodes:
•    Preorder, where we visit the node, then visit the left and right subtrees
•   Inorder, where we visit the left subtree, then visit the node, then visit the right subtree
•    Postorder, where we visit the left and right subtrees, then visit the node

### 5.7 Recursive Binary-Tree Algorithms· 235

Our goal is to build a tournament: a binary tree where the item in every internal node is a copy of the larger of the items in its two children. In particular, the item at the root is a copy of the largest item in the tournament. The items in the leaves (nodes with no children) constitute the data of interest, and the rest of the tree is a data structure that allows us to find the largest of the items efficiently.

### 5.8 Graph Traversal· 241

Depth-first search requires time proportional to V +E in a graph with V vertices and E edges, using the adjacency lists representation.

### 5.9 Perspective· 247

Recursion lies at the heart of early theoretical studies into the nature of computation. Recursive functions and programs playa central role in mathematical studies that attempt to separate problems that can be solved by a computer from problems that cannot be.

## Chapter 6. Elementary Sorting Methods 253

As a rule, the elementary methods that we discuss here take time proportional to N^2 to sort N randomly arranged items.

### 6.1 Rules of the Game· 255

We shall be considering methods of sorting files of items containing keys.The keys, which are only part (often a small part) of the items, are used to control the sort. The objective of the sorting method is to rearrange the items such that their keys are ordered according to some well-defined ordering rule (usually numerical or alphabetical order).

A sorting method is said to be stable if it preserves the relative order of items with duplicated keys in the file.

### 6.2 Selection Sort· 261

Selection sort works by repeatedly selecting the smallest remaining element.

### 6.3 Insertion Sort ' 262

The method that people often use to sort bridge hands is to consider the elements one at a time, inserting each into its proper place among those already considered (keeping them sorted).

### 6.4 Bubble Sort· 265

Keep passing through the file, exchanging adjacent elements that are out of order, continuing until the file is sorted.

### 6.5 Performance Characteristics of Elementary Sorts ' 267

Selection sort uses about N^2/2 comparisons and N exchanges.

Insertion sort uses about N^2/4 comparisons and N^2/4 half-exchanges (moves) on the average, and twice that many at worst.

Bubble sort uses about N^2/2 comparisons and N^2/2 exchanges on the average and in the worst case.

An inversion is a pair of keys that are out of order in the file.

Insertion sort and bubble sort use a linear number of comparisons and exchanges for files with at most a constant number of inversions corresponding to each element.

Insertion sort uses a linear number of comparisons and exchanges for files with at most a constant number of elements having more than a constant number of corresponding inversions.

Selection sort runs in linear time for files with large items and small keys.

### 6.6 Shellsort ' 273

The idea is to rearrange the file to give it the property that taking every hth element (starting anywhere) yields a sorted file. Such a file is said to be h-sorted.

Our primary purpose in doing so is to illustrate that even algorithms that are apparently simple can have complex properties, and that the analysis of algorithms is not just of practical importance but also can be intellectually challenging.

The result of h-sorting a file that is k-ordered is a file that is both h- and k-ordered. 

Shellsort does less than N(h-1)(k-1)/g comparisons to g-sort a file that is h- and k-ordered, provided that hand k are relatively prime. 

Shellsort does less than O(N^(3/2)) comparisons for the increments 1 4 13 40 121 364 1093 3280 9841 .... 

Shellsort does less than O(N^(4/3)) comparisons for the
increments 1 8 23 77 281 1073 4193 16577 .... 

### 6.7 Sorting Other Types of Data, 281

We consider implementations, interfaces, and client programs for:
• Items, or generic objects to be sorted
• Arrays of items
The item data type provides us with a way to use our sort code for any type of data for which certain basic operations are defined.

### 6.8 Index and Pointer Sorting, 287

One simple approach for sorting without (intermediate) moves of items is to maintain an index array with keys in the items accessed only for comparisons. 

### 6.9 Sorting of Linked Lists ' 294

The possibility that nodes could be referenced through pointers that are maintained outside the sort means that our programs should change only links in nodes, and should not alter keys or other information. 

### 6,10 Key-Indexed Counting, 298

Key-indexed counting is a linear-time sort, provided that the range of distinct key values is within a constant factor of the file size. 

## Chapter 7. Quicksort 303

### 7.1 The Basic Algorithm. 304

### 7.2 Performance Characteristics of Quicksort, 309

### 7.3 Stack Size· 313

### 7.4 SmallSubfiles' 316

### 7.5 Median-of-Three Partitioning· 319

### 7.6 Duplicate Keys· 324

### 7.7 Strings and Vectors· 327

### 7.8 Selection· 329

## Chapter 8. Merging and Mergesort 335

### 8.1 Two-Way Merging· 336

### 8.2 Abstract In-place Merge, 339

### 8.3 Top-Down Mergesort . 341

### 8.4 Improvements to the Basic Algorithm, 344

### 8.5 Bottom-Up Mergesort . 347

### 8.6 Performance Characteristics of Mergesort . 351

### 8.7 Linked-List Implementations of Mergesort . 354

### 8.8 Recursion Revisited, 357

## Chapter 9. Priority Queues and Heapsort 361

### 9.1 Elementary Implementations· 365

### 9.2 Heap Data Structure, 368

### 9.3 Algorithms on Heaps· 371

### 9.4 Heapsort . 376

### 9.5 Priority-Queue ADT . 383

### 9.6 Priority Queues for Index Items· 389

### 9.7 Binomial Queues· 392

## Chapter 10. Radix Sorting 403

### 10.1 Bits, Bytes, and Words· 405

### 10.2 Binary Quicksort· 409

### 10.3 MSD Radix Sort· 413

### 10.4 Three-Way Radix Quicksort· 421

### 10.5 LSD Radix Sort· 425

### 10.6 Performance Characteristics of Radix Sorts· 428

### 10.7 Sublinear-Time Sorts· 433

## Chapter 11. Special-Purpose Sorts 439

### 11.1 Batcher's Odd-Even Mergesort . 441

### 11.2 Sorting Networks . 446

### 11.3 External Sorting . 454

### 11.4 Sort-Merge Implementations· 460

### 11.5 Parallel Sort/Merge . 466

## Chapter 12. Symbol Tables and BSTs

### 12.1 Symbol-Table Abstract Data Type· 479

### 12.2 Key-Indexed Search· 485

### 12.3 Sequential Search· 489

### 12.4 Binary Search· 497

### 12.5 Binary Search Trees (BSTs) . 502

### 12.6 Performance Characteristics of BSTs . 508

### 12.7 Index Implementations with Symbol Tables· 511

### 12.8 Insertion at the Root in BSTs . 516

### 12.9 BST Implementations of Other ADT Functions· 519

## Chapter 13. Balanced Trees 529

### 13.1 Randomized BSTs . 533

### 13.2 Splay BSTs . 540

### 13.3 Top-Down 2-3-4 Trees· 546

### 13.4 Red-Black Trees· 5.51

### 13.5 Skip Lists . .561

### 13.6 Performance Characteristics· .569

## Chapter 14. Hashing 573

### 14.1 Hash Functions . .574

### 14.2 Separate Chaining' .583

### 14.3 Linear Probing . S88

### 14.4 Double Hashing . 594

### 14.5 Dynamic Hash Tables· .599

### 14.6 Perspective· 603

## Chapter 15. Radix Search 609

### 15.1 Digital Search Trees 610

### 15.2 Tries· 614

### 15.3 Patricia Tries· 623

### 15.4 Multiway Tries and TSTs . 632

### 15.5 Text String Index Algorithms· 648

## Chapter 16. External Searching 655

### 16.1 Rules of the Game· 6.57

### 16.2 Indexed Sequential Access· 660

### 16.3 B Trees· 662

### 16.4 Extendible Hashing· 676

### 16.5 Perspective· 688

